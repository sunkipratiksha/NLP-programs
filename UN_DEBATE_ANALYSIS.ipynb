{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunkipratiksha/NLP-programs/blob/main/UN_DEBATE_ANALYSIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rH7SBX3OPqgI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysMp3OO_P63c"
      },
      "outputs": [],
      "source": [
        "# Check the content in the directory\n",
        "!ls '/content/drive/My Drive/NLP PROGRAMS'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grbj8Y3yP68I"
      },
      "outputs": [],
      "source": [
        "# Unzip the file\n",
        "!unzip '/content/drive/My Drive/UNGDC_1970-2018.zip'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geu-U0RFP7B_"
      },
      "outputs": [],
      "source": [
        "# Check the content in the converted sessions directory\n",
        "!ls 'Converted sessions'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX93PKq1P7FA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Check the content in the specific session directory\n",
        "!ls 'Converted sessions/Session 25 - 1970'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKK850qrQNQq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW4R-rtjQNTe"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Retrieve the list of folders matching the pattern 'Converted sessions/Session*'\n",
        "folders = glob.glob('Converted sessions/Session*')\n",
        "print(folders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgIViM8ZQNWW"
      },
      "outputs": [],
      "source": [
        "# Create an empty dataframe with columns\n",
        "df = pd.DataFrame(columns=['COUNTRY', 'SPEECH', 'SESSION', 'YEAR'])\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU9bm91qQNZK"
      },
      "outputs": [],
      "source": [
        "i=0\n",
        "\n",
        " #iterates over each folder path in the folders list.\n",
        "for file in folders:\n",
        "\n",
        "  #glob.glob() function - again to search for speech file paths that match the pattern file+\"/USA*.txt\" i.e  start with 'USA' and end with '.txt' \n",
        "  speech = glob.glob(file+\"/USA*.txt\")\n",
        "\n",
        "  #The code opens the first speech file in the speech list using open(speech[0], encoding=\"utf-8\")\n",
        "  # Populate the dataframe with speech data\n",
        "  with open(speech[0],encoding=\"utf-8\") as f:\n",
        "    df.loc[i,'SPEECH'] = f.read()\n",
        "    df.loc[i,'YEAR'] = speech[0].split('_')[-1].split('.')[0]\n",
        "    df.loc[i,'SESSION'] =speech[0].split('_')[-2]\n",
        "    df.loc[i,'COUNTRY'] = speech[0].split('_')[0].split(\"/\")[-1]\n",
        "    i+=1\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecdJJtXrQgG3"
      },
      "outputs": [],
      "source": [
        "df['SPEECH'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcoxEOTeQgSA"
      },
      "outputs": [],
      "source": [
        "# TEXT CLEANING\n",
        "def cleaned(text):\n",
        "    # Remove new line characters\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    # Remove punctuations except periods(.) and question marks(?)\n",
        "    text = re.sub(r\"[^\\w\\s.?]\", '', text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\t', '', text)\n",
        "    text = re.sub(r'^\\d+\\.', '', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "#assigning the result of applying the cleaned function to the 'Speech' column of the DataFrame df to a new column called 'CLEANED SPEECH'.\n",
        "df['CLEANED SPEECH'] = df['SPEECH'].apply(cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "un5ua-kJQgVX"
      },
      "outputs": [],
      "source": [
        "print(df['CLEANED SPEECH'])  #this is cleaned speech\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qiwe-uyYT6RB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# SENTENCE SEGMENTATION\n",
        "def sentences(text):\n",
        "    # #SPLITTING INTO SENTENCES BY FULLSTOPS OR QUEST MARKS(?)\n",
        "    text = re.split('[.?]', text)\n",
        "    clean_sent = []\n",
        "    for sent in text:\n",
        "        clean_sent.append(sent)\n",
        "    return clean_sent\n",
        "\n",
        "#assigning the result of applying the \"sentences\" function to the 'CLEANED SPEECH' column of the DataFrame df to a new column called 'SENTENCES'.\n",
        "df[\"SENTENCES\"] = df['CLEANED SPEECH'].apply(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwwouOeBQgYb"
      },
      "outputs": [],
      "source": [
        "print(df[\"SENTENCES\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tmrc747FQgbU"
      },
      "outputs": [],
      "source": [
        "# Print a sample sentence from the 'SENTENCES' column\n",
        "df[\"SENTENCES\"][1]    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW195fv3WUE9"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE6vvhqkWpaF"
      },
      "outputs": [],
      "source": [
        "# CREATE A DATAFRAME THAT CONTAINS SENTENCES AND ITS WORDCOUNT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1JdRxWjYUp1"
      },
      "outputs": [],
      "source": [
        " # CREATE A DATAFRAME THAT CONTAINS SENTENCES AND ITS WORDCOUNT\n",
        "df2 = pd.DataFrame(columns=['YEAR','SENTENCES','WORDCOUNT'])\n",
        "row_list=[]\n",
        "\n",
        "# Iterate over each row in the dataframe\n",
        "for i in range(len(df)):\n",
        "  #iterate over each sentence from 'SENTENCE' column of the current row:\n",
        "  for sent in df.loc[i,'SENTENCES']:\n",
        "    wordcount = len(sent.split())   #  splits the sentence at each whitespace character (space, tab, newline) and returns a list of the individual words in the sentence.\n",
        "    year = df.loc[i,'YEAR']\n",
        "    # Create a dictionary with sentence, word count, and year\n",
        "    dict1 = {'YEAR': year, 'SENTENCES': sent, 'WORDCOUNT': wordcount}\n",
        "    # Append the dictionary to the row_list\n",
        "    row_list.append(dict1)\n",
        "\n",
        "df2 = pd.DataFrame(row_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysQTG3o0WplT"
      },
      "outputs": [],
      "source": [
        "# Display the first few rows of df2\n",
        "print(df2.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0mfv5FUg7ET"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RKgDMD7D7IP"
      },
      "source": [
        "#Sample texts from dataset\n",
        "\n",
        "1. For that reason, **President Reagan**, in his speech to this body last year, proposed that the United States and the Soviet Union exchange visits of experts at test sites to measure directly the yields of nuclear weapon test\n",
        "\n",
        "2. **President Reagan** has directed our scientists and engineers to examine, in the light of new technologies and fully in accord with the Anti Ballistic Missile Treaty the feasibility of defense against ballistic missile attack\n",
        "\n",
        "3. **President Reagan** approach to foreign policy is grounded squarely on standards drawn from the pragmatic American experience 4. Let me start by joining the **President of Brazi**l in conveying to the people and Government of Mexico our deep sympathy over the devastation wrought by earthquakes and our solidarity with them as they work to recover and rebuild"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD5_T45WhHdK"
      },
      "outputs": [],
      "source": [
        "#IMPORT SPACY AND MATCHER LIBRARY\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyvUJHO2hOS5"
      },
      "outputs": [],
      "source": [
        "# Define a function to find the president's name in a given text\n",
        "def find(text):\n",
        "  names=[]           # Initialize an empty list to store the names of presidents\n",
        "  doc = nlp(text)   # Process the sentence with the loaded spaCy model\n",
        "\n",
        "\n",
        "  #pattern :\n",
        "  pattern1 = [\n",
        "      {'LOWER':'president'},   # Match the lowercase word 'president'\n",
        "      {'POS':'ADP','OP':'?'},   # Match an optional adposition (preposition or subordinating conjunction)\n",
        "      {'POS':'PROPN'}             # Match a proper noun (name)\n",
        "  ]\n",
        "\n",
        "  #1. Initialize spacy Matcher object:\n",
        "  m = Matcher(nlp.vocab)\n",
        "\n",
        "  #2. Add defined pattern to the Matcher object(m):\n",
        "  m.add('1',[pattern1])\n",
        "\n",
        "  #3. pattern matching on the processed sentence\n",
        "  matches = m(doc)\n",
        "\n",
        "  for matches_id,start,end in matches:    # Iterate over the matched patterns\n",
        "    n = doc[start:end]    # Extract the matched span from the document\n",
        "\n",
        "    names.append(n.text)   # Append the text of the matched span to the names list\n",
        "  return names   # Return the list of president names found in the sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY_Jwm4xh_kH"
      },
      "outputs": [],
      "source": [
        "# Extract presidents' names from the sentences in df2\n",
        "extracted = []\n",
        "for i in range(df2.shape[0]):\n",
        "    extracted.append(find(df2['SENTENCES'][i]))\n",
        "df2['PRESIDENT']=extracted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMRd-I-JS33L"
      },
      "outputs": [],
      "source": [
        "print(df2.columns) \n",
        "#the new column 'PRESIDENT' is added in df2 dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvxibS-VVGnK"
      },
      "outputs": [],
      "source": [
        "print(df2.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdulbuf81HB4"
      },
      "source": [
        "here the **'PRESIDENT'** column is null, since all sentences may not contain the pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4UbwtusVTcy"
      },
      "outputs": [],
      "source": [
        "#mention all the non-null outputs :\n",
        "mention = []\n",
        "for i in range(df2.shape[0]):\n",
        "  if df2['PRESIDENT'][i]!=[]:\n",
        "    mention.append(df2['PRESIDENT'][i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KubzCw0oVdd2"
      },
      "outputs": [],
      "source": [
        "mention  #shows all presidents name "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG3CNejmVmIV"
      },
      "outputs": [],
      "source": [
        "# Create a list to store sentences containing presidents' names\n",
        "president_sent = []\n",
        "for i in range(df2.shape[0]):\n",
        "    if df2['PRESIDENT'][i] != []:\n",
        "        president_sent.append(df2['SENTENCES'][i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-BQbzpzV6-m"
      },
      "outputs": [],
      "source": [
        "president_sent[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbPk5zIYWJBS"
      },
      "outputs": [],
      "source": [
        "len(president_sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eWBhO5u0xR3"
      },
      "source": [
        "......................................................................................................................................................................................................................................\n",
        "\n",
        "Sample sentences from dataset:\n",
        "these are the important programs used in the speech:\n",
        "\n",
        "1. The United States will continue its strong efforts to advance **the United Nations plan** for Namibia\n",
        "\n",
        "2. The United States and Italy have proposed **a Global Peace Operations Initiative**\n",
        "\n",
        "3. For 35 years, **the North Atlantic alliance** has guaranteed the peace in Europe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DQ0ra771miL"
      },
      "source": [
        "**keywords**: plan , programme , scheme , compaign ,  initiative , conference , \n",
        "          agreement , alliance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AAqX0gU2BmG"
      },
      "source": [
        "pattern: \n",
        "1.   the | DET | det\n",
        "2.   United | PROPN | compound\n",
        "3.   Nations | PROPN | compound\n",
        "4.   plan | NOUN | dobj\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3QpQfriWlVL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Define a function to check if keywords are present in a text\n",
        "def keywords(text):\n",
        "    #words = ['plan', 'programme', 'scheme', 'campaign', 'initiative', 'conference', 'agreement', 'alliance']\n",
        "    words = [r'\\b(?i)'+'plan'+r'\\b',\n",
        "              r'\\b(?i)'+'programme'+r'\\b',\n",
        "              r'\\b(?i)'+'scheme'+r'\\b',\n",
        "              r'\\b(?i)'+'campaign'+r'\\b',\n",
        "              r'\\b(?i)'+'initiative'+r'\\b',\n",
        "              r'\\b(?i)'+'conference'+r'\\b',\n",
        "              r'\\b(?i)'+'agreement'+r'\\b',\n",
        "              r'\\b(?i)'+'alliance'+r'\\b'] \n",
        "\n",
        "    output=[]\n",
        "    count = 0\n",
        "    for i in words:\n",
        "        if re.search(i, text) != None:\n",
        "            count = 1\n",
        "            break\n",
        "    return count\n",
        "\n",
        "# Add a new column 'KEYWORDS' to df2 indicating if keywords are present in the sentences\n",
        "df2['KEYWORDS'] = df2['SENTENCES'].apply(keywords)\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3mojaKwYPo7"
      },
      "outputs": [],
      "source": [
        "print(df2.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVQvY39hYMlq"
      },
      "outputs": [],
      "source": [
        "# Display the first few rows of df2\n",
        "df2.head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgqIgldjYbey"
      },
      "source": [
        "here the **'KEYWORDS'** column is null, since all sentences may not contain the keywords defined in the list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28I318ZRYtVG"
      },
      "outputs": [],
      "source": [
        "# identify specific keywords related to schemes or initiatives mentioned in the text.\n",
        "\n",
        "def all_keywords(text,check):\n",
        "    schemes = []   # Initialize an empty list to store the extracted keywords\n",
        "\n",
        "    doc = nlp(text)   # Process the input text with the loaded spaCy model\n",
        "    word_list = ['plan', 'programme', 'scheme', 'campaign', 'initiative', 'conference', 'agreement', 'alliance']\n",
        "\n",
        "    # Define the pattern to match the keywords\n",
        "    pattern2 = [\n",
        "        {'POS': 'DET'},                        # Matches a determiner\n",
        "        {'POS': 'PROPN', 'DEP': 'compound'},      # Matches a proper noun with a compound dependency\n",
        "        {'POS': 'PROPN', 'DEP': 'compound'},        # Matches a proper noun with a compound dependencY\n",
        "        {'POS': 'PROPN', 'OP': '?'},              #Matches an optional proper noun\n",
        "        {'POS': 'PROPN', 'OP': '?'},               #Matches an optional proper noun\n",
        "        {'POS': 'PROPN', 'OP': '?'},               #Matches an optional proper noun\n",
        "       {'LOWER': {'IN': word_list}, 'OP': '+'}         # Matches any of the specified keywords one or more times\n",
        "    ] \n",
        "\n",
        "    if check==0:\n",
        "      return schemes #return blank list\n",
        "\n",
        "    # Initialize spacy Matcher object\n",
        "    m = Matcher(nlp.vocab)\n",
        "\n",
        "    # Add defined pattern to the Matcher object\n",
        "    m.add('pattern2', [pattern2])\n",
        "\n",
        "    # Perform pattern matching on the processed text\n",
        "    matches = m(doc)\n",
        "\n",
        "    for i in range(0,len(matches)):  # get the match ID, the starting position, and the ending position of the match.\n",
        "        start = matches[i][1]\n",
        "        end = matches[i][2]\n",
        "        if doc[start].pos_ == 'DET':  #  checks if the word at the starting position of the match is a determiner (like \"the,\" \"a,\" or \"an\").\n",
        "\n",
        "            #  If the word at the starting position is a determiner, we increment the starting position by 1. \n",
        "            # This is because we want to skip the determiner and start the keyword from the next word.\n",
        "            start += 1\n",
        "\n",
        "        # creates a string called \"span\" by joining the words from the starting position to the ending position of the match. It represents the extracted keyword from the text.\n",
        "        span = str(doc[start:end])\n",
        "\n",
        "        # checks if the schemes list is not empty and \n",
        "        # if the last keyword in the list is already present in the current span. It helps us avoid adding duplicate keywords.\n",
        "        if (len(schemes) != 0) and (schemes[-1] in span):\n",
        "\n",
        "            #  If the last keyword in the list is already present in the current span, we update it with the new span.\n",
        "            schemes[-1] = span\n",
        "\n",
        "        #  If the current span is not a duplicate, we add it to the schemes list.    \n",
        "        else:\n",
        "            schemes.append(span)\n",
        "\n",
        "    return schemes\n",
        "\n",
        "#df2['SCHEMES1'] = df2.apply(lambda x: all_keywords(x['SENTENCES'], x['KEYWORDS']), axis=1)\n",
        "\n",
        "# Add a new column 'SCHEMES1' to df2 containing the extracted keywords\n",
        "df2['SCHEMES1']=df2.apply(lambda x: all_keywords(x.SENTENCES,x.KEYWORDS),axis=1)      \n",
        "\n",
        "#df2['SCHEMES1'] = df2['KEYWORDS'].astype(str).apply(all_keywords)\n",
        "\n",
        "#astype() function in the code is used to change the data type of a column in a pandas DataFrame\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LM11eeGsKibj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f6r1NphcoXZ"
      },
      "outputs": [],
      "source": [
        "print(df2.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6mZYa0RdKql"
      },
      "outputs": [],
      "source": [
        "df2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmYZc_tUgNu4"
      },
      "outputs": [],
      "source": [
        "df2['KEYWORDS']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6Lxvv-6hcez"
      },
      "outputs": [],
      "source": [
        "mention = []  # Initialize an empty list to store mentions\n",
        "\n",
        "if 'SCHEMES1' in df2.columns:  # Check if 'SCHEMES1' column exists in df2\n",
        "    \n",
        "    for i in range(df2.shape[0]):      # Iterate over each row in df2\n",
        "       \n",
        "        if df2['SCHEMES1'][i] != []:            # Check if the value in the 'SCHEMES1' column at index i is not an empty list\n",
        "           \n",
        "            mention.append(df2['SCHEMES1'][i])     # Append the value to the 'mention' list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ_l7PBah1KC"
      },
      "outputs": [],
      "source": [
        "mention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS6x5R9Gl_qR"
      },
      "outputs": [],
      "source": [
        "df2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d84Bnn7k0n1P"
      },
      "outputs": [],
      "source": [
        "print(df2.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND-OBY4wmLE2"
      },
      "outputs": [],
      "source": [
        "initiatives = []  # Initialize an empty list to store initiatives\n",
        "\n",
        "# Iterate over each row in df2\n",
        "for i in range(df2.shape[0]):\n",
        "\n",
        "    # Check if the value in the 'SCHEMES1' column at index i is not an empty list\n",
        "    if df2['SCHEMES1'][i] != []:\n",
        "      \n",
        "        # Append the value in the 'SENTENCES' column at index i to the 'initiatives' list\n",
        "        initiatives.append(df2['SENTENCES'][i])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlVpf9oEnWUQ"
      },
      "outputs": [],
      "source": [
        "initiatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGbBlnifnd2z"
      },
      "outputs": [],
      "source": [
        "len(initiatives)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrhFfrkXxvs8"
      },
      "source": [
        "# RELATION EXTRACTION PART 1:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18JASzQdojOd"
      },
      "source": [
        "**Sample sentences from dataset:**\n",
        "\n",
        "1. The United **States** will **support** these **principles**\n",
        "\n",
        "2. Yet these very small **entities need** more than most the **assistance** that the United Nations system can provide\n",
        "\n",
        "3. I have proposed to Congress that the United **States provide** additional **funding** for our work in Iraq, the greatest financial commitment of its kind since the Marshall Plan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2s6sGIs-_ap"
      },
      "outputs": [],
      "source": [
        "def RULE_1(text):\n",
        "  doc=nlp(text)    # uses the spaCy model (nlp) to process the input text and store the result in the variable doc\n",
        "  sent=[]         #sent is initialized to store the extracted phrases.\n",
        "  for token in doc:   # starts a loop that iterates over each token in the processed doc.\n",
        "    if token.pos_=='VERB':   #checks if the current token's part-of-speech (POS) tag is 'VERB'. If it is, the code proceeds to the next steps. Otherwise, it skips the token.\n",
        "      phrase=\"\"      #empty string phrase to build the extracted phrase for the current verb.\n",
        "      for sub_token in token.lefts:   #starts a loop that iterates over the left children of the current verb token.\n",
        "        if (sub_token.dep_ in ['nsubj','nsubjpass'] and (sub_token.pos_ in ['NOUN','PROPN','PRON'])):  #checks if the current left child's dependency label is either 'nsubj' or 'nsubjpass', and \n",
        "                                                                                                        #its POS tag is either 'NOUN' or 'PROPN'. If both conditions are satisfied, the code proceeds to the next steps. Otherwise, it skips the left child.\n",
        "          \n",
        "          phrase += sub_token.text + ' ' + token.lemma_   #  appends the text of the left child, followed by a space and the lemma (base form) of the current verb token, to the phrase string.\n",
        "\n",
        "          for sub_token in token.rights:    #starts a loop that iterates over the right children of the current verb token.\n",
        "\n",
        "            if (sub_token.dep_ in ['dobj']) and (sub_token.pos_ in ['NOUN','PROPN']):   #checks if the current right child's dependency label is 'dobj' and its POS tag is either 'NOUN' or 'PROPN'.\n",
        "                                                                                         #If both conditions are satisfied, the code proceeds to the next steps. Otherwise, it skips the right child.\n",
        "              phrase+=' '+sub_token.text   # appends a space and the text of the right child to the phrase string.\n",
        "              sent.append(phrase)  # appends the completed phrase to the sent list.\n",
        "  return sent    # Returns the sent list containing all the extracted phrases for the given text.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "rowlist=[]  #used to store information about each row.\n",
        "for i in range(len(df2)):   #execute once for each row in df2.\n",
        "\n",
        "  sent=df2.loc[i,'SENTENCES']    # takes the sentence from the 'SENTENCES' column and assigns it to the variable 'sent'. This represents the sentence for that row.\n",
        "  year = df2.loc[i,'YEAR']         #  takes the year from the 'YEAR' column and assigns it to the variable 'year'. This represents the year for that row.\n",
        "  output = RULE_1(sent)         # extract1() function is called with the sentence 'sent' as input\n",
        "                                  # This function analyzes the sentence and extracts relevant information.\n",
        "                                  # The extracted information is stored in the output variable as a list.\n",
        "\n",
        "  dict1 = {'YEAR':year,'SENTENCES':sent,'OUTPUT':output}   # A dictionary called dict1 is created to hold the year, sentence, and extracted information.\n",
        "  rowlist.append(dict1)  # dict1 dictionary is added to the rowlist.\n",
        "\n",
        "df3 = pd.DataFrame(rowlist)  # rowlist is converted into a DataFrame called df3\n",
        "#print(df3.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "HARbIg4UIzqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df3.head())"
      ],
      "metadata": {
        "id": "qBClqWtmOSJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_show = pd.DataFrame(columns = df3.columns)  # An empty DataFrame called df_show is created with the same columns as the df3 DataFrame.\n",
        "\n",
        "for row in range(len(df3)):  # The loop will execute once for each row in df3.\n",
        "\n",
        "  if len(df3.loc[row,'OUTPUT'])!=0:         # checks if the length of the 'OUTPUT' column in the current row of df3 is not equal to zero.\n",
        "    df_show = df_show.append(df3.loc[row,:])  # If the condition is true, it means that the row has non-empty output, so the entire row is appended to the df_show DataFrame.\n",
        "\n",
        "df_show.reset_index(inplace=True, drop=True)   # the index of the df_show DataFrame is reset, and the inplace parameter is set to True,\n",
        "                                               # which means the operation is performed on the DataFrame itself rather than returning a new DataFrame.\n",
        "\n"
      ],
      "metadata": {
        "id": "7eflQemVNvPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_show.head())"
      ],
      "metadata": {
        "id": "8d2mNsRmOnTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.columns"
      ],
      "metadata": {
        "id": "F1R0ADjxQS38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_show.columns"
      ],
      "metadata": {
        "id": "1uBjGXXLIr1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_show.shape"
      ],
      "metadata": {
        "id": "6BYc0nIqO340"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#SEPARATE SUBJECT, VERB AND OBJECT\n",
        "verb_dict= {}   #used to store the count of each verb encountered.\n",
        "separated_components = {}  # used to store the separated components (subject, verb, and object) of each sentence.\n",
        "components = []\n",
        "for i in range(len(df_show)):   # iterates over the rows of the DataFrame df_show.\n",
        "\n",
        "  #  retrieve the values of the 'SENTENCES', 'YEAR', and 'OUTPUT' columns for the current row of df_show\n",
        "  sent = df_show.loc[i,'SENTENCES']\n",
        "  year = df_show.loc[i,'YEAR']\n",
        "  output = df_show.loc[i,'OUTPUT']\n",
        "\n",
        "  for sent in output:  # iterates over each sentence in the output list.\n",
        "  \n",
        "    n1 = sent.split()[0]   # Split the sentence into words and assign the first word to n1 (subject)\n",
        "    v = sent.split()[1]      # Split the sentence into words and assign the second word to v (verb)\n",
        "    n2 = sent.split()[2]   # Split the sentence into words and assign the remaining words to n2 (object)\n",
        "\n",
        "    # Create a dictionary to store the separated components for the current sentence\n",
        "    separated_components = {'YEAR':year,'SENTENCES':sent,'NOUN1':n1 , 'VERB':v , 'NOUN2':n2}\n",
        "\n",
        "    # Append the separated components dictionary to the list of components\n",
        "    components.append(separated_components)\n",
        "\n",
        "    # Count the occurrence of the verb in the verb dictionary\n",
        "\n",
        "    verb = sent.split()[1]\n",
        "    if verb in verb_dict:\n",
        "      verb_dict[verb]+=1\n",
        "    else:\n",
        "      verb_dict[verb]=1\n",
        "\n",
        "df_sep = pd.DataFrame(components)\n",
        "df_sep.head(10)\n",
        "  "
      ],
      "metadata": {
        "id": "ucLMxb1yQaJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sep[df_sep['VERB']=='support'].head()"
      ],
      "metadata": {
        "id": "4RM_V-V1Vpck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sep['VERB'].value_counts()[0:10]\n"
      ],
      "metadata": {
        "id": "kzRiqJ9Ol5me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBDPpxfObOJU"
      },
      "source": [
        "# RELATION EXTRACTION PART 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample sentences from the dataset:\n",
        "\n",
        "1. With support from **many countries**, we have made **impressive progress**\n",
        "\n",
        "2. Because of their **unique expertise** and regional legitimacy, they can be instruments for solving some of the **hardest challenges** we face\n",
        "\n",
        "3. We are right to aim high and take on the **mightiest tasks**"
      ],
      "metadata": {
        "id": "R2AHkQtJagTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(nlp(\"With support from many countries, we have made impressive progress\"),style='dep',jupyter=True)\n",
        "doc = nlp(\"With support from many countries, we have made impressive progress\")\n"
      ],
      "metadata": {
        "id": "hxpHinRB-JKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RULE_2(text):\n",
        "  doc = nlp(text)\n",
        "  pattern = []  #Initialize an empty list to store the extracted patterns.\n",
        "  for token in doc:\n",
        "    phrase = ''  # Initialize an empty string to store the extracted pattern for the current token.\n",
        "\n",
        "    #Check if the current token is a noun and its dependency is one of the specified values ('dobj', 'pobj', 'nsubj', 'nsubjpass')\n",
        "    #This condition ensures that we consider nouns that function as objects or subjects.\n",
        "    if ((token.pos_ == 'NOUN') and (token.dep_ in ['dobj','pobj','nsubj','nsubjpass'])):\n",
        "\n",
        "      #Iterate over the children of the current token.\n",
        "      # used to check each word and ,if that word  has any other words directly connected to it.\n",
        "      for subtoken in token.children:\n",
        "\n",
        "        #Check if the child token is an adjective or has a \"compound\" dependency.\n",
        "        if(subtoken.pos_=='ADJ') or (subtoken.dep_=='compound'):\n",
        "\n",
        "          #If the child token satisfies the condition, concatenate its text with the existing phrase, separated by a space.\n",
        "          phrase = phrase + subtoken.text+' '\n",
        "\n",
        "      if len(phrase)!=0:    # Check if the phrase is not empty.\n",
        "        phrase = phrase + token.text+' '\n",
        "\n",
        "    # If the phrase is not empty, concatenate the current token's text to the phrase, separated by a space.\n",
        "    if len(phrase)!=0:\n",
        "\n",
        "      # If the phrase is not empty, add it to the list of patterns.\n",
        "      pattern.append(phrase)\n",
        "\n",
        "  return pattern\n"
      ],
      "metadata": {
        "id": "z2_iG-lNbVab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.columns"
      ],
      "metadata": {
        "id": "UiW0LPd-gUhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rowlist = []   # Initialize an empty list rowlist to store the dictionaries for each row of the new DataFrame.\n",
        "\n",
        "for i in range(len(df2)):   #to get the index values.\n",
        "  year = df2.loc[i,'YEAR']\n",
        "  sent = df2.loc[i,'SENTENCES']\n",
        "\n",
        "  output = RULE_2(sent)  # Call the extract2 function on the current sentence (sent) to extract adjective-noun patterns and store in the output variable.\n",
        "\n",
        "  dict1 = {'YEAR':year,'SENTENCES':sent , 'OUTPUT':output}  # Create a dictionary dict1 containing the 'YEAR', 'SENTENCES', and 'OUTPUT' values.\n",
        "  rowlist.append(dict1)\n",
        "\n",
        "df4 = pd.DataFrame(rowlist)   # create a new DataFrame df4 using pd.DataFrame(rowlist), which converts the list of dictionaries into a DataFrame.\n",
        "df4.head()"
      ],
      "metadata": {
        "id": "X0B7LS1kgLsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SELECTING NON-EMPTY OUTPUTS:\n",
        "\n",
        "df_show = pd.DataFrame(columns = df4.columns)\n",
        "for row in range(len(df4)):\n",
        "  if (len(df4.loc[row,'OUTPUT'])!=0):        # checks if the length of the 'OUTPUT' column in the current row of df4 is not equal to zero\n",
        "    df_show = df_show.append(df4.loc[row,:])   # If the condition is true, it means that the row has non-empty output, so the entire row is appended to the df_show DataFrame.\n",
        "\n",
        "df_show.reset_index(inplace=True, drop=True)   # the index of the df_show DataFrame is reset, and the inplace parameter is set to True,\n",
        "                                               # which means the operation is performed on the DataFrame itself rather than returning a new DataFrame."
      ],
      "metadata": {
        "id": "5QdXvK0nH4S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_show.shape\n",
        "df_show.head()"
      ],
      "metadata": {
        "id": "7cG4GyHhI6L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining Rule 1 and Rule 2"
      ],
      "metadata": {
        "id": "Eom7GPZ8OAFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample sentences:\n",
        "\n",
        "1. With support from many countries, **we** have **made** **Impressive progress**\n",
        "\n",
        "2. **It** will **require** **military planners** the world over to recognize that training for peace operations is a legitimate part of every nation security strategy"
      ],
      "metadata": {
        "id": "e13nIlwyJhza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MODIFIYING RULE 2\n",
        "def RULE_2_mod(text,index):\n",
        "  doc = nlp(text)\n",
        "  phrase = \"\"\n",
        "\n",
        "  for token in doc:\n",
        "    # For each word, the function checks if its position (token.i) matches the provided index.\n",
        "    #If there is a match, it means we found the word we are looking for.\n",
        "    #If there is no match, we move on to the next word in the document.\n",
        "    if token.i == index:   \n",
        "\n",
        "      #Once we find the target word, we look at its children (words that are connected to it in the sentence).\n",
        "      for subtoken in token.children:\n",
        "\n",
        "        #For each child word, we check if it is an adjective (subtoken.pos_ == 'ADJ') or if it has a compound relationship with the target word (subtoken.dep_ == 'compound').\n",
        "        if (subtoken.pos_ == 'ADJ') or (subtoken.dep_=='compound'):\n",
        "\n",
        "          #If the child word meets either of these conditions, we add its text to the phrase variable.\n",
        "          phrase = phrase + subtoken.text\n",
        "\n",
        "      # we stop the loop using the break statement since we have found what we needed.\n",
        "      break\n",
        "\n",
        "  # Finally, the function returns the phrase, which contains the adjectives that describe the target word.    \n",
        "  return phrase"
      ],
      "metadata": {
        "id": "j2qiNelKJ17Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"This code essentially looks for VERBS in the text and checks if they have subject nouns and direct object nouns associated with them. \n",
        "It then extracts any adjectives related to these nouns and forms a pattern using the adjective, noun, and verb lemma.  \"\"\"\n",
        "\n",
        "#USING RULE 3 AND RULE 4\n",
        "def RULE_1_mod(text):\n",
        "  doc= nlp(text)\n",
        "  sent=[]  # create an empty list called sent to store our extracted patterns.\n",
        "\n",
        "  for token in doc:\n",
        "\n",
        "    #  check if the current token has a part-of-speech tag 'VERB'\n",
        "    if (token.pos_=='VERB'):\n",
        "      phrase=''   # initialize an empty string called phrase to store the pattern.\n",
        "\n",
        "      # left children of the verb token using token.lefts -  tokens that appear before the verb in the sentence.\n",
        "      for sub_tok in token.lefts:\n",
        "        if (sub_tok.dep_ in ['nsubj','nsubjpass']) and (sub_tok.pos_ in ['NOUN','PROPN','PRON']):\n",
        "\n",
        "          # if above conditions are satisfied - call the extract2_mod function to extract the adjective (if any) associated with the subject noun.\n",
        "          # pass the text and the index of the subject noun (sub_tok.i) to the extract2_mod function.\n",
        "          adj = RULE_2_mod(text,sub_tok.i)\n",
        "\n",
        "          #  concatenate the extracted adjective, the subject noun, and a space to the phrase.\n",
        "          phrase += adj + ' ' + sub_tok.text\n",
        "          \n",
        "          #  concatenate the lemma of the verb (token.lemma_) to the phrase. The lemma is the base or dictionary form of the verb.\n",
        "          phrase += ' '+token.lemma_\n",
        "\n",
        "          # Right children are the tokens that appear after the verb in the sentence.\n",
        "          for sub_tok in token.rights:\n",
        "            if(sub_tok.dep_ in ['dobj']) and (sub_tok.pos_ in ['NOUN','PROPN']):\n",
        "\n",
        "              # if above conditions are satisfied - call the extract2_mod function to extract the adjective (if any) associated with the subject noun.\n",
        "              # pass the text and the index of the subject noun (sub_tok.i) to the extract2_mod function.\n",
        "              adj = RULE_2_mod(text,sub_tok.i)\n",
        "\n",
        "              #  concatenate the extracted adjective, the subject noun, and a space to the phrase.\n",
        "              phrase += adj + ' '+sub_tok.text\n",
        "\n",
        "              # append the phrase to the sent list.\n",
        "              sent.append(phrase)\n",
        "  return sent"
      ],
      "metadata": {
        "id": "cs04DrNiVykx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rowlist = []\n",
        "\n",
        "for i in range(len(df2)):\n",
        "  year = df2.loc[i,'YEAR']\n",
        "  sent = df2.loc[i,'SENTENCES']\n",
        "\n",
        "  output = RULE_1_mod(sent)\n",
        "\n",
        "  dict1 = {'YEAR':year,'SENTENCES':sent , 'OUTPUT':output}\n",
        "  rowlist.append(dict1)\n",
        "\n",
        "df_rule = pd.DataFrame(rowlist)\n",
        "df_rule.head()"
      ],
      "metadata": {
        "id": "VLjdyU5-X_5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SELECTING NON-EMPTY OUTPUTS\n",
        "df_show_mod = pd.DataFrame(columns = df_rule.columns)\n",
        "\n",
        "for row in range(len(df_rule)):\n",
        "  if len(df_rule.loc[row,'OUTPUT'])!=0:\n",
        "    df_show_mod = df_show_mod.append(df_rule.loc[row,:])\n",
        "\n",
        "\n",
        "#reset the index\n",
        "df_show_mod.reset_index(inplace=True, drop=True)\n"
      ],
      "metadata": {
        "id": "smGTUaGxYTbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_show_mod.shape"
      ],
      "metadata": {
        "id": "eX9qYh416znw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_show_mod.head()"
      ],
      "metadata": {
        "id": "XBjZfEedtT9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_show_mod.loc[4,'OUTPUT'])\n",
        "print(df_show_mod.loc[4,'SENTENCES'])"
      ],
      "metadata": {
        "id": "iTrtVTMztZOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_show_mod.loc[12,'OUTPUT'])\n",
        "print(df_show_mod.loc[12,'SENTENCES'])"
      ],
      "metadata": {
        "id": "8ndtd8RgupZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rule 5: Patterns using prepositions"
      ],
      "metadata": {
        "id": "Ub_rCVIHxyIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample sentences:\n",
        "\n",
        "1. We all believe the **benefits of globalization** must be allocated more broadly within and among societies\n",
        "\n",
        "2. As the Millennium Summit reflected, we have no **shortage of** worthy **goals**\n",
        "\n",
        "3. It should preserve the special **responsibility for peacekeeping** of the Security Council permanent members"
      ],
      "metadata": {
        "id": "vPa-83b4xePQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(nlp(\"We all believe the benefits of globalization must be allocated more broadly within and among societies\"),style='dep',jupyter=True)\n"
      ],
      "metadata": {
        "id": "-YjbCgsI5rP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RULE_3(text):\n",
        "  doc = nlp(text)\n",
        "  sent = []\n",
        "\n",
        "  for token in doc:\n",
        "    if token.pos_=='ADP':\n",
        "      phrase = ''\n",
        "      if token.head.pos_=='NOUN':\n",
        "        phrase += token.head.text\n",
        "        phrase += ' '+token.text\n",
        "\n",
        "        for right_token in token.rights:\n",
        "          if (right_token.pos_ in ['NOUN','PROPN']):\n",
        "            phrase += ' '+right_token.text\n",
        "\n",
        "        if (len(phrase)>2):\n",
        "          sent.append(phrase)\n",
        "\n",
        "  return sent\n",
        "\n"
      ],
      "metadata": {
        "id": "UfEYclT1x_gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rowlist = []\n",
        "\n",
        "for i in range(len(df2)):\n",
        "  year = df2.loc[i,'YEAR']\n",
        "  sent = df2.loc[i,'SENTENCES']\n",
        "\n",
        "  output = RULE_3(sent)\n",
        "\n",
        "  dict1 = {'YEAR':year,'SENTENCES':sent , 'OUTPUT':output}\n",
        "  rowlist.append(dict1)\n",
        "\n",
        "df_rule = pd.DataFrame(rowlist)\n",
        "df_rule.head()"
      ],
      "metadata": {
        "id": "G3q6XRUb-8d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SELECTING NON-EMPTY OUTPUTS:\n",
        "\n",
        "df_show = pd.DataFrame(columns = df_rule.columns)\n",
        "\n",
        "for row in range(len(df_rule)):\n",
        "  if len(df_rule.loc[row,'OUTPUT'])!=0:\n",
        "    df_show = df_show.append(df_rule.loc[row,:])\n",
        "\n",
        "\n",
        "#reset the index\n",
        "df_show.reset_index(inplace=True, drop=True)\n"
      ],
      "metadata": {
        "id": "jY1w0Q0hBXxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_show.head(10)"
      ],
      "metadata": {
        "id": "wv_KXSkKCWhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_show.columns"
      ],
      "metadata": {
        "id": "v3OACjExD4dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SEPARATE NOUN, PREPOSITION AND NOUN\n",
        "prep_dict={}      # used to store the count of each preposition encountered\n",
        "separated_components ={}   # used to store the separated components (noun1, prepo , noun2)\n",
        "components = []   \n",
        "\n",
        "for i in range(len(df_show)):\n",
        "  # retrieve the values of these columns for the current row of df_show dataframe\n",
        "  sent = df_show.loc[i,'SENTENCES']\n",
        "  year = df_show.loc[i,'YEAR']\n",
        "  output = df_show.loc[i,'OUTPUT']\n",
        "\n",
        "  for sent in output:         #iterated over each sentence in output list\n",
        "\n",
        "    n1 = sent.split()[0]\n",
        "    p = sent.split()[1]\n",
        "    n2 = sent.split()[2:] \n",
        "\n",
        "    separated_components = {'YEAR':year,'SENTENCES':sent,'NOUN-1':n1,'PREPOSITION':p,'NOUN-2':n2}\n",
        "\n",
        "    components.append(separated_components)\n",
        "\n",
        "    prep = sent.split()[1]\n",
        "    if prep in prep_dict:\n",
        "      prep_dict[prep]+=1\n",
        "    else:\n",
        "      prep_dict[prep]=1\n",
        "\n",
        "df_sep2 = pd.DataFrame(components)\n"
      ],
      "metadata": {
        "id": "GSACGxfAC-KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sep2.head(10)"
      ],
      "metadata": {
        "id": "u8j3TwKUF54v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sep2['PREPOSITION'].value_counts()[:10]"
      ],
      "metadata": {
        "id": "Ovomq7RwGTTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sep2[df_sep2['PREPOSITION']=='against'].head(10)"
      ],
      "metadata": {
        "id": "2d5IXPr-GjsB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoPxCa0hBnZB7SpQIZewYu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}